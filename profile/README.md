<div align="center">
  <img src="../logo/logo.png"  alt="Lab Logo">
  
  <h1>Embodied AI Safety Lab</h1>
  
  <p>
    <strong>Ensuring the Robustness, Security, and Alignment of Intelligent Agents in the Physical World.</strong>
  </p>

  <p>
    <a href="https://your-lab-website.com">
      <img src="https://img.shields.io/badge/Website-Visit_Us-blue?style=flat&logo=safari" alt="Website">
    </a>
    <a href="https://twitter.com/your_lab">
      <img src="https://img.shields.io/badge/Twitter-Follow_Us-1DA1F2?style=flat&logo=twitter" alt="Twitter">
    </a>
    <a href="mailto:contact@your-lab.edu">
      <img src="https://img.shields.io/badge/Email-Contact_Us-D14836?style=flat&logo=gmail" alt="Email">
    </a>
     <img src="https://img.shields.io/badge/Focus-Embodied_AI_Safety-success?style=flat" alt="Focus">
  </p>
</div>

---

## üî¨ About Us

Welcome to the **Embodied AI Safety Lab**. We are dedicated to bridging the gap between advanced artificial intelligence and safe physical deployment. Our research focuses on identifying vulnerabilities in robotic perception and control, developing robust defense mechanisms, and ensuring human-robot value alignment.

### üõ°Ô∏è Our Core Research Areas
| **Robust Perception** | **Safe Reinforcement Learning** | **Sim2Real Transfer** | **Adversarial Security** |
|:---:|:---:|:---:|:---:|
| üëÅÔ∏è | üß† | üåâ | ‚öîÔ∏è |
| Defending vision systems against environmental perturbations and attacks. | Constrained exploration and safe policy optimization for robotics. | Ensuring safety guarantees hold when moving from simulation to reality. | Testing physical agents against adversarial examples and sensor spoofing. |

---

## üì¢ News & Updates

* **[2024-05]** üéâ Two papers accepted to **ICRA 2024** regarding safe navigation!
* **[2024-02]** üèÜ Our paper on *Adversarial Robotic Vision* won the Best Paper Award at the AI Safety Workshop.
* **[2023-12]** üöÄ We have released the `SafeBench` simulation environment v1.0.

---

## üìö Featured Publications

### 2024
* **"Towards Provable Safety in Embodied Agents via Control Barrier Functions"**
    * *Alice Chen, Bob Smith, Carol Zhang*
    * **ICRA 2024**
    * [üìÑ PDF](#) | [üíª Code](#) | [üåê Project Page](#)

* **"Attack-Resilient Visual Servoing for Manipulators"**
    * *David Lee, Alice Chen*
    * **CVPR 2024**
    * [üìÑ PDF](#) | [üíª Code](#) | [üåê Project Page](#)

### 2023
* **"Benchmarking Robustness in Vision-Language Navigation"**
    * *Team Member A, Team Member B*
    * **NeurIPS 2023 (Oral)**
    * [üìÑ PDF](#) | [üíª Code](#) | [üåê Project Page](#)

---

## üíª Open Source Projects

We believe in open science. Here are some of our maintained repositories:

| Project | Description | Tech Stack |
| :--- | :--- | :--- |
| **[ü§ñ RoboGuard](#)** | An adversarial attack toolbox specifically designed for robotic perception pipelines. | `Python` `PyTorch` `ROS` |
| **[üõë Safety-Gym-Real](#)** | A suite of safety-critical benchmarks for real-world quadruped robots. | `MuJoCo` `Isaac Gym` |
| **[üß† Verifiable-RL](#)** | Formal verification tools for deep reinforcement learning policies. | `C++` `Python` |

> *Check our [Repositories](https://github.com/orgs/YourOrgName/repositories) tab for a complete list of our work.*

---

## üë• Team

We are a diverse team of researchers, engineers, and students passionate about AI Safety.

<table>
  <tr>
    <td align="center"><img src="https://github.com/github.png" width="60px;" alt=""/><br /><sub><b>Prof. Name</b></sub><br /><i>Principal Investigator</i></td>
    <td align="center"><img src="https://github.com/github.png" width="60px;" alt=""/><br /><sub><b>Researcher A</b></sub><br /><i>Postdoc</i></td>
    <td align="center"><img src="https://github.com/github.png" width="60px;" alt=""/><br /><sub><b>Student B</b></sub><br /><i>PhD Candidate</i></td>
    <td align="center"><img src="https://github.com/github.png" width="60px;" alt=""/><br /><sub><b>Student C</b></sub><br /><i>PhD Candidate</i></td>
    <td align="center"><img src="https://github.com/github.png" width="60px;" alt=""/><br /><sub><b>Student D</b></sub><br /><i>Master Student</i></td>
  </tr>
</table>

---

## ü§ù Join Us

We are always looking for motivated students and collaborators interested in **Trustworthy Embodied AI**.
If you are interested in working with us:
1.  Read our [Recent Publications](#).
2.  Check out our [Open Positions](#).
3.  Send an email to `contact@your-lab.edu` with your CV and a brief statement of interest.

<div align="center">
  <br>
  <sub>&copy; 2024 Embodied AI Safety Lab. All rights reserved.</sub>
</div>

